{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bubblepuzzle/ProductSummarization/blob/main/Major_Project_FINAL_DRAFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEfnVigmtHu"
      },
      "source": [
        "DATASET EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-A_Xu2Um2OJ",
        "outputId": "987cbc6b-7d4b-478a-c29c-51e8244bc13d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import random\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "! pip install split-folders\n",
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIBk_X_NHXPl",
        "outputId": "32f77484-27ee-4842-96cc-3b38fcdaee90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CnfesdtttDI8BuE-Mxpg5c3LZETdpu2b\n",
            "From (redirected): https://drive.google.com/uc?id=1CnfesdtttDI8BuE-Mxpg5c3LZETdpu2b&confirm=t&uuid=d3cb70cd-89f7-434e-964b-fe98bdc202e1\n",
            "To: /content/amazon_reviews_us_Baby_v1_00.tsv.zip\n",
            "100% 362M/362M [00:03<00:00, 96.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1CnfesdtttDI8BuE-Mxpg5c3LZETdpu2b\n",
        "import zipfile as zf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YeRv4DuHvUJ"
      },
      "outputs": [],
      "source": [
        "hehe=zf.ZipFile(\"amazon_reviews_us_Baby_v1_00.tsv.zip\",'r')\n",
        "hehe.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6G22UEHm2en"
      },
      "outputs": [],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytJyD14yocEg"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/REVIEW Major Proj\"\n",
        "dataset_dir = os.path.join(root_path, \"DataSet\")\n",
        "dataset_path= os.path.join(dataset_dir, \"amazon_reviews_us_Baby_v1_00.tsv.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vbPFa8HpP-2"
      },
      "outputs": [],
      "source": [
        "#Decompress and load the tsv in a pandas data frame\n",
        "reviews = pd.read_csv('/content/amazon_reviews_us_Baby_v1_00.tsv.zip', compression=\"zip\", sep=\"\\t\", header=0, error_bad_lines=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mSNJYvapQCh"
      },
      "outputs": [],
      "source": [
        "reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEUjvCujqD9O"
      },
      "outputs": [],
      "source": [
        "sample = reviews.head(200000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9elWN4uhN-tc"
      },
      "outputs": [],
      "source": [
        "product_review_counts = reviews['product_id'].value_counts()\n",
        "\n",
        "# Get the product IDs for the top 5 products with the highest review counts\n",
        "top_products = product_review_counts.head(5).index.tolist()\n",
        "\n",
        "# Create a subset of the dataset containing reviews for the top products\n",
        "subset_reviews = reviews[reviews['product_id'].isin(top_products)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inxLlgwPOF5P"
      },
      "outputs": [],
      "source": [
        "subset_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZW18_ne-Z3u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "subset_reviews['review_year'] = pd.to_datetime(subset_reviews['review_date']).dt.year\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(subset_reviews.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM7EfWrO_gTl"
      },
      "outputs": [],
      "source": [
        "subset_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awQlTMV3OTo_"
      },
      "outputs": [],
      "source": [
        "subset_reviews.to_csv('/content/Reduced DataSet.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqezwhNFOzuW"
      },
      "source": [
        "We're using a subset of the starting dataset for simplicity, in particular it formed by the reviews of the 5 products with the highest number of reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhnQ4yK2qELp"
      },
      "outputs": [],
      "source": [
        "sample.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDgBocy6qPFS"
      },
      "outputs": [],
      "source": [
        "#Save the sample into a csv on the shared drive\n",
        "sample.to_csv('/content/sample.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qINKuhRSqjXv"
      },
      "source": [
        "DATA Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z35MXj8qv_T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stopwords_en = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHA4mOq2rA3m"
      },
      "source": [
        "TEXT PREPROCESSING\n",
        "\n",
        "In the following we've defined a function that applies some text pre processing techniques:\n",
        "\n",
        "Transform to lowercase\n",
        "\n",
        "Remove the HTML tags\n",
        "\n",
        "Tokenization\n",
        "\n",
        "Stopwords removal\n",
        "\n",
        "Remove the punctuation marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg5dL8Xeq7h8"
      },
      "outputs": [],
      "source": [
        "#Creation of a list of reviews, each review will be tokenized considering a word\n",
        "# as a token.\n",
        "def preprocess_review(review_list):\n",
        "\n",
        "  #regex to remove HTML tags\n",
        "  html_removal = re.compile('<.*?>')\n",
        "\n",
        "  cleaned_reviews = []\n",
        "  for review in review_list:\n",
        "      #each review must be converted to lowercase\n",
        "      lower_review = review.lower()\n",
        "\n",
        "      #remove the html tags\n",
        "      nohtml_review = re.sub(html_removal, ' ', lower_review)\n",
        "\n",
        "      #tokenized the review using the word_tonkenize\n",
        "      tokenized_review = word_tokenize(nohtml_review)\n",
        "\n",
        "      #stopwords removal\n",
        "      nostopwords_review = []\n",
        "\n",
        "      #for each token, check if the token is inside the list of stopwords\n",
        "      # for token in tokenized_review:\n",
        "      #     if token not in stopwords_en:\n",
        "      #         #append the ones not in the list of stopwords\n",
        "      #         nostopwords_review.append(token)\n",
        "      nostopwords_review = [token for token in tokenized_review if token not in stopwords_en]\n",
        "\n",
        "      #remove punctuation #1\n",
        "      #remove a token only if it is composed by a single punctuation mark\n",
        "      #nopunctuation_review = [token for token in nostopwords_review if token not in string.punctuation]\n",
        "\n",
        "      #remove punctuation #2\n",
        "      #remove tokens composed entirely by punctuation marks\n",
        "      nopunctuation_review = [token for token in nostopwords_review if not all(char in string.punctuation for char in token)]\n",
        "\n",
        "      #remove punctuation #3 #we don't need to use tagging for punctuation!\n",
        "      # pos_tagging = nltk.pos_tag(nostopwords_review)\n",
        "      # nopunctuation_review = [tuple for tuple in pos_tagging if len(tuple[1]) > 1 ]\n",
        "\n",
        "      cleaned_reviews.append(' '.join(nopunctuation_review))\n",
        "\n",
        "  return cleaned_reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX9YGatbq746"
      },
      "outputs": [],
      "source": [
        "#Function to write the list into a file\n",
        "def write_list_to_file(filename, list):\n",
        "  #Opening a file in 'w' mode clears its content\n",
        "  file = open(filename,'w')\n",
        "  for item in list:\n",
        "    file.write(item+\"\\n\")\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBgZmMp8rky2"
      },
      "outputs": [],
      "source": [
        "#Function to read a list from a file\n",
        "def read_list_from_file(filename):\n",
        "  result = []\n",
        "  # opening the file in read mode\n",
        "  my_file = open(filename, \"r\")\n",
        "\n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "\n",
        "  # replacing end splitting the text\n",
        "  # when newline ('\\n') is seen.\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "  return data_into_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8NkVbj2rqBy"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive to access the shared folder\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp2xrVigrz8Y"
      },
      "outputs": [],
      "source": [
        "#Load the dataset as csv, skipping the bad lines\n",
        "csv = pd.read_csv(\"/content/Reduced DataSet.csv\", on_bad_lines=\"skip\")\n",
        "\n",
        "#Drop the null values\n",
        "csv.dropna(inplace=True)\n",
        "\n",
        "#Check the number of the reviews for each product in the reduced dataset\n",
        "csv_grouped = csv.groupby(by=\"product_id\").count().sort_values(by=\"review_body\")\n",
        "csv_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvKup6xjtk1y"
      },
      "outputs": [],
      "source": [
        "ax = csv_grouped.plot.bar(y='review_id', rot=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BcgGAgYO-7y"
      },
      "source": [
        "Dataset creation\n",
        "\n",
        "For the test of the models we'll use just the reviews of the product with product_id B000YDDF6O, the features that we use are just the review_body and review_headline, the latter is used as label during the fine tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djYTLdEEu2L_"
      },
      "outputs": [],
      "source": [
        "csv_r = csv[csv[\"product_id\"] == \"B000YDDF6O\"]\n",
        "csv_r = csv_r[[\"review_body\", \"review_headline\"]]\n",
        "csv_r.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o81XvF8Du2p6"
      },
      "outputs": [],
      "source": [
        "csv_r.to_csv('/content/amazon_reviews_reduced_preprocessed.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP2D6ft_u25S"
      },
      "outputs": [],
      "source": [
        "#TEST\n",
        "csv_r = pd.read_csv(\"/content/amazon_reviews_reduced_preprocessed.csv\", on_bad_lines=\"skip\")\n",
        "#OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r56pb69PP-SW"
      },
      "outputs": [],
      "source": [
        "#Sample the reviews to be summarized\n",
        "review_list = csv_r[\"review_body\"].tail(1000).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGbGPhuJP-U3"
      },
      "outputs": [],
      "source": [
        "#Preprocess the list\n",
        "review_list = preprocess_review(review_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6mblcOFP-YZ"
      },
      "outputs": [],
      "source": [
        "#Filename for the review list's file\n",
        "filename_review_list = \"/content/review_list.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m16gRf5tQPsa"
      },
      "outputs": [],
      "source": [
        "#Save the list to the disk\n",
        "write_list_to_file(filename_review_list, review_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7XMuxyHQP31"
      },
      "outputs": [],
      "source": [
        "#Check if the list was correctly written\n",
        "res = read_list_from_file(filename_review_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdcou7mCQQBu"
      },
      "outputs": [],
      "source": [
        "#Check if thw two lists are the same list\n",
        "all(x == y for x, y in zip(review_list, res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nelvg3gvQbcR"
      },
      "source": [
        "Analyzing the reviews composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cre25eN7QZ04"
      },
      "outputs": [],
      "source": [
        "#Dict containing the number of tokens as key and the number of reviews having\n",
        "# such a number of tokens\n",
        "lengths = dict()\n",
        "for review in review_list:\n",
        "  length = len(review.split(\" \"))\n",
        "  if length in lengths:\n",
        "    lengths[length] += 1\n",
        "  else:\n",
        "    lengths[length] = 1\n",
        "\n",
        "#Sort in increasing number of keys\n",
        "sorted_lengths = dict(sorted(lengths.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xFUxtJfQaCL"
      },
      "outputs": [],
      "source": [
        "plt.xlabel ('Number of tokens')\n",
        "plt.ylabel ('Number of reviews')\n",
        "plt.bar(sorted_lengths.keys(), sorted_lengths.values(), color='b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpppteEWQqhW"
      },
      "outputs": [],
      "source": [
        "print(\"(number of tokens: {}, number of reviews: {})\".format(max(lengths, key= lambda x: lengths[x]), lengths[max(lengths, key= lambda x: lengths[x])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UfNETwNS30f"
      },
      "outputs": [],
      "source": [
        "total = 0\n",
        "#Sum the lengths of the reviews\n",
        "for review in review_list:\n",
        "  total += len(review.split(\" \"))\n",
        "\n",
        "#Divide by the total number of reviews\n",
        "avg = total / len(review_list)\n",
        "\n",
        "print(\"Avg length of review: {}\".format( avg) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alnSWU4GTa4c"
      },
      "source": [
        "**T5 Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdXkZNaUTYQd"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official\n",
        "!pip install -q transformers\n",
        "!pip install -q spacy\n",
        "!pip install -q datasets\n",
        "#!pip install -q pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zOn2PXBTgxM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#import tensorflow_models as tfm\n",
        "#import tensorflow_hub as hub\n",
        "#import tensorflow_datasets as tfds\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "from datasets import Dataset\n",
        "import datasets\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stopwords_en = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umXtJh7ITg4v"
      },
      "outputs": [],
      "source": [
        "#from huggingface_hub import notebook_login\n",
        "\n",
        "#notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "see2oCLKThHV"
      },
      "outputs": [],
      "source": [
        "csv = pd.read_csv(\"drive/MyDrive/REVIEW Major Proj/DataSet/Reduced DataSet.csv\", on_bad_lines=\"skip\")\n",
        "csv.dropna(inplace=True)\n",
        "csv.groupby(by=\"product_id\").count().sort_values(by=\"review_body\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzkXy4IxUQm3"
      },
      "outputs": [],
      "source": [
        "csv_r = csv[csv[\"product_id\"] == \"B000YDDF6O\"]\n",
        "csv_r = csv_r[[\"review_body\", \"review_headline\"]]\n",
        "csv_r.reset_index(inplace=True, drop=True)\n",
        "review_headline = csv_r[\"review_headline\"].tail(1000).tolist()\n",
        "review_list = csv_r[\"review_body\"].tail(1000).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-dgKubZUQ0u"
      },
      "outputs": [],
      "source": [
        "review_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M40SMPNqUf1K"
      },
      "outputs": [],
      "source": [
        "dataset = ds.dataset(pa.Table.from_pandas(csv_r).to_batches())\n",
        "dataset = Dataset(pa.Table.from_pandas(csv_r[:2000]))\n",
        "dataset = dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NyIDTNKVRW7"
      },
      "outputs": [],
      "source": [
        "p = dataset.get(\"train\")\n",
        "max_l = 0\n",
        "for elem in p:\n",
        "  max_l = max_l + len(elem[\"review_body\"])\n",
        "max_l/1000.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIo7N247VRZ-"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"review_body\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"review_headline\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re-hRFCcVRdY"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV8yObP5VdQe"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"facebook/bart-base\", return_tensors=\"tf\")\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE09Y15LVdTO"
      },
      "outputs": [],
      "source": [
        "#from transformers.keras_callbacks import PushToHubCallback\n",
        "#model.compile(optimizer=optimizer)\n",
        "\n",
        "#model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO-M5CrwVdWl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "#Creation of a list of reviews, each review will be tokenized considering a word\n",
        "# as a token.\n",
        "\n",
        "print(review_list[0])\n",
        "\n",
        "#regex to remove HTML tags\n",
        "html_removal = re.compile('<.*?>')\n",
        "\n",
        "cleaned_reviews = []\n",
        "for review in review_list:\n",
        "    #each review must be converted to lowercase\n",
        "    lower_review = review.lower()\n",
        "\n",
        "    #remove the html tags\n",
        "    nohtml_review = re.sub(html_removal, ' ', lower_review)\n",
        "\n",
        "    #tokenized the review using the word_tonkenize\n",
        "    tokenized_review = word_tokenize(nohtml_review)\n",
        "\n",
        "    #stopwords removal\n",
        "    nostopwords_review = []\n",
        "\n",
        "    #for each token, check if the token is inside the list of stopwords\n",
        "    # for token in tokenized_review:\n",
        "    #     if token not in stopwords_en:\n",
        "    #         #append the ones not in the list of stopwords\n",
        "    #         nostopwords_review.append(token)\n",
        "    nostopwords_review = [token for token in tokenized_review if token not in stopwords_en]\n",
        "\n",
        "    #remove punctuation #1\n",
        "    #remove a token only if it is composed by a single punctuation mark\n",
        "    #nopunctuation_review = [token for token in nostopwords_review if token not in string.punctuation]\n",
        "\n",
        "    #remove punctuation #2\n",
        "    #remove tokens composed entirely by punctuation marks\n",
        "    nopunctuation_review = [token for token in nostopwords_review if not all(char in string.punctuation for char in token)]\n",
        "\n",
        "    #remove punctuation #3 #we don't need to use tagging for punctuation!\n",
        "    # pos_tagging = nltk.pos_tag(nostopwords_review)\n",
        "    # nopunctuation_review = [tuple for tuple in pos_tagging if len(tuple[1]) > 1 ]\n",
        "\n",
        "    cleaned_reviews.append(' '.join(nopunctuation_review))\n",
        "\n",
        "print(cleaned_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD5ez1b_XmiJ"
      },
      "outputs": [],
      "source": [
        "# reviews = []\n",
        "# for i in range(0, 400):\n",
        "#     sequence = cleaned_reviews[i]\n",
        "#     inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "#     output = model.generate(inputs, max_length=100)\n",
        "#     summary=tokenizer.decode(output[0])\n",
        "#     reviews.append(summary)\n",
        "# seq = ' '.join(reviews)\n",
        "# inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "# output = model.generate(inputs, max_length=100)\n",
        "# summary=tokenizer.decode(output[0])\n",
        "# print(\"T5 summary nltk: \" + summary)\n",
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "\n",
        "\n",
        "#batch creation\n",
        "\n",
        "\n",
        "batches = []\n",
        "i = 0\n",
        "for i in range(0,100):\n",
        "  if count + len(cleaned_reviews[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + cleaned_reviews[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(cleaned_reviews[i].split(\" \"))\n",
        "  else:\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + cleaned_reviews[i] + \" . \"\n",
        "    count = len(cleaned_reviews[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cNe_aSYX9l8"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2CFQI3DX910"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr6YBSpcYCi2"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bZpgT5JYHQN"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efqHwpH1YHSo"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6voeUXPYHWE"
      },
      "outputs": [],
      "source": [
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = review_list[i]\n",
        "    inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model.generate(inputs, max_length=100)\n",
        "    summary=tokenizer.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model.generate(inputs, max_length=100)\n",
        "summary=tokenizer.decode(output[0])\n",
        "print(\"T5 summary total: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = cleaned_reviews[i]\n",
        "    inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model.generate(inputs, max_length=100)\n",
        "    summary=tokenizer.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model.generate(inputs, max_length=100)\n",
        "summary=tokenizer.decode(output[0])\n",
        "print(\"T5 summary nltk: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = fin[i]\n",
        "    inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model.generate(inputs, max_length=100)\n",
        "    summary=tokenizer.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model.generate(inputs, max_length=100)\n",
        "summary=tokenizer.decode(output[0])\n",
        "print(\"T5 summary spacy: \" + summary)\n",
        "\n",
        "#device = cuda.get_current_device()\n",
        "#device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfyGutigYSjI"
      },
      "outputs": [],
      "source": [
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = review_list[i]\n",
        "    inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model.generate(inputs, max_length=100)\n",
        "    summary=tokenizer.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model.generate(inputs, max_length=100)\n",
        "summary=tokenizer.decode(output[0])\n",
        "print(\"bart-base summary total: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = nltk_fin[i]\n",
        "    inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model.generate(inputs, max_length=100)\n",
        "    summary=tokenizer.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model.generate(inputs, max_length=100)\n",
        "summary=tokenizer.decode(output[0])\n",
        "print(\"bart-base summary nltk: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = fin[i]\n",
        "    inputs  = tokenizer.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model.generate(inputs, max_length=100)\n",
        "    summary=tokenizer.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model.generate(inputs, max_length=100)\n",
        "summary=tokenizer.decode(output[0])\n",
        "print(\"bart-base summary spacy: \" + summary)\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl17oqgbheC0"
      },
      "source": [
        "\n",
        "**STOPWORD ELIMINATION WITH NLTK AND TEXT REDUCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xegNNGs6hfmU"
      },
      "outputs": [],
      "source": [
        "nltk_test = []\n",
        "for elem in review_list:\n",
        "    nltk_test.append(word_tokenize(elem))\n",
        "\n",
        "nltk_un = []\n",
        "for text in nltk_test:\n",
        "    temp = []\n",
        "    for w in text:\n",
        "        if w not in stop_words:\n",
        "            temp.append(w)\n",
        "    nltk_un.append(' '.join(temp))\n",
        "\n",
        "nltk_fin = []\n",
        "for elem in nltk_un:\n",
        "    if len(elem) <= 1000:\n",
        "        nltk_fin.append(elem)\n",
        "\n",
        "rev = []\n",
        "for doc in review_list:\n",
        "    words = []\n",
        "    for token in nlp(doc):\n",
        "        if (token.pos_ not in ['VERB', 'ADJ', 'NOUN', 'DET', 'PUNCT']):\n",
        "        #if (token.pos_ == 'VERB' or token.pos_ == 'ADJ' or token.pos_ == 'NOUN' or token.pos_ == 'DET' or token.pos_ == 'PUNCT'):\n",
        "            words.append(token.lemma_)\n",
        "    rev.append(words)\n",
        "\n",
        "fin = []\n",
        "for doc in rev:\n",
        "    fin.append(\" \".join(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKrRjYS_htg0"
      },
      "source": [
        "**T5 TEST WITH FULL TEXT, NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0jUayThhu-9"
      },
      "outputs": [],
      "source": [
        "tokenizer_t5 = AutoTokenizer.from_pretrained('t5-base')\n",
        "model_t5 = TFAutoModelForSeq2SeqLM.from_pretrained('t5-base', return_dict=True)\n",
        "\n",
        "print()\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = review_list[i]\n",
        "    inputs  = tokenizer_t5.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model_t5.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_t5.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_t5.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model_t5.generate(inputs, max_length=100)\n",
        "summary=tokenizer_t5.decode(output[0])\n",
        "print(\"T5 summary total: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = nltk_fin[i]\n",
        "    inputs  = tokenizer_t5.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model_t5.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_t5.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_t5.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model_t5.generate(inputs, max_length=100)\n",
        "summary=tokenizer_t5.decode(output[0])\n",
        "print(\"T5 summary nltk: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = fin[i]\n",
        "    inputs  = tokenizer_t5.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model_t5.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_t5.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_t5.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model_t5.generate(inputs, max_length=100)\n",
        "summary=tokenizer_t5.decode(output[0])\n",
        "print(\"T5 summary spacy: \" + summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccxK5xuchztu"
      },
      "source": [
        "**BART TEST WITH FULL TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhI_i_cZhvGl"
      },
      "outputs": [],
      "source": [
        "tokenizer_bart = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model_bart = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn', return_dict=True)\n",
        "\n",
        "print()\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = review_list[i]\n",
        "    inputs  = tokenizer_bart.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model_bart.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_bart.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_bart.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model_bart.generate(inputs, max_length=100)\n",
        "summary=tokenizer_bart.decode(output[0])\n",
        "print(\"Bart summary total: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = nltk_fin[i]\n",
        "    inputs  = tokenizer_bart.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model_bart.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_bart.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_bart.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model_bart.generate(inputs, max_length=100)\n",
        "summary=tokenizer_bart.decode(output[0])\n",
        "print(\"Bart summary nltk: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = fin[i]\n",
        "    inputs  = tokenizer_bart.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=1024)\n",
        "    output = model_bart.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_bart.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_bart.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=1024)\n",
        "output = model_bart.generate(inputs, max_length=100)\n",
        "summary=tokenizer_bart.decode(output[0])\n",
        "print(\"Bart summary spacy: \" + summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA9kDW7jgnoE"
      },
      "source": [
        "**PEGASUS TEST WITH FULL TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHVU9NOHm276"
      },
      "outputs": [],
      "source": [
        "tokenizer_pega = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "model_pega = TFAutoModelForSeq2SeqLM.from_pretrained('google/pegasus-xsum', return_dict=True)\n",
        "\n",
        "print()\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = review_list[i]\n",
        "    inputs  = tokenizer_pega.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=512)\n",
        "    output = model_pega.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_pega.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_pega.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=512)\n",
        "output = model_pega.generate(inputs, max_length=100)\n",
        "summary=tokenizer_pega.decode(output[0])\n",
        "print(\"Pegasus summary totale: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = nltk_fin[i]\n",
        "    inputs  = tokenizer_pega.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=512)\n",
        "    output = model_pega.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_pega.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_pega.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=512)\n",
        "output = model_pega.generate(inputs, max_length=100)\n",
        "summary=tokenizer_pega.decode(output[0])\n",
        "print(\"Pegasus summary nltk: \" + summary)\n",
        "\n",
        "reviews = []\n",
        "for i in range(0, 50):\n",
        "    sequence = fin[i]\n",
        "    inputs  = tokenizer_pega.encode(\"summarize: \" + sequence, return_tensors='tf', truncation=True, max_length=512)\n",
        "    output = model_pega.generate(inputs, max_length=100)\n",
        "    summary=tokenizer_pega.decode(output[0])\n",
        "    reviews.append(summary)\n",
        "seq = ' '.join(reviews)\n",
        "inputs  = tokenizer_pega.encode(\"summarize: \" + seq, return_tensors='tf', truncation=True, max_length=512)\n",
        "output = model_pega.generate(inputs, max_length=100)\n",
        "summary=tokenizer_pega.decode(output[0])\n",
        "print(\"Pegasus summary spacy: \" + summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurtftUjbsrX"
      },
      "source": [
        "**T5 BASE FINETUNING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kViovwem3O6"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "554qwNkd9M1Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqMWwk649NGU"
      },
      "outputs": [],
      "source": [
        "#Function to read a list from a file\n",
        "def read_list_from_file(filename):\n",
        "  result = []\n",
        "  # opening the file in read mode\n",
        "  my_file = open(filename, \"r\")\n",
        "\n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "\n",
        "  # replacing end splitting the text\n",
        "  # when newline ('\\n') is seen.\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "  return data_into_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynEkv9Pd9Uw8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zQ2S8KUHU0K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLjfLQvGavaG"
      },
      "outputs": [],
      "source": [
        "#Filename of the review list's file\n",
        "filename_review_list = \"/content/review_list.txt\"\n",
        "\n",
        "#Filename of the dataset\n",
        "filename_dataset = \"/content/Reduced DataSet.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA7IRdXhavc2"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(filename_dataset, on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSELsBR5avf8"
      },
      "outputs": [],
      "source": [
        "amazon_reviews.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nxA4PgibiGd"
      },
      "source": [
        "DEFINITION OF FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW-KFdhMbm5V"
      },
      "outputs": [],
      "source": [
        "def get_dataset_from_product_id(df, product_id, number_of_samples, test_size, shuffle):\n",
        "  csv_r = df[df[\"product_id\"] == product_id]\n",
        "  csv_r = csv_r[[\"review_body\", \"review_headline\"]]\n",
        "  csv_r = csv_r[csv_r[\"review_headline\"] != \"Five Stars\"]\n",
        "\n",
        "  #TODO\n",
        "  # review_body and review_headline must be preprocessed\n",
        "\n",
        "  dataset = Dataset.from_pandas(df = csv_r.sample(number_of_samples))\n",
        "  dataset = dataset.train_test_split(test_size = test_size)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2kgZ4sLbm8J"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # inputs = [prefix + doc for doc in examples[\"review_body\"]]\n",
        "    inputs = [doc for doc in examples[\"review_body\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"review_headline\"], max_length=256, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F9Q4qJ7bm_V"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  i = np.argmin(val_loss)\n",
        "  x_min = epochs[i]\n",
        "  y_min = val_loss[i]\n",
        "  plt.plot(x_min, y_min,'g',marker='o', label=\"Minimum validation loss\")\n",
        "\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaUbuYcCb6Xx"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ49NuUAb_jE"
      },
      "source": [
        "**TEST TRAINING 1000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TaPpWp0cDmb"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset_from_product_id(amazon_reviews, \"B000YDDF6O\", 1000, 0.2, True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93FXKuP6cDzz"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjIZZrqjcqJg"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "print(tokenized_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muN2z-D9cqa0"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "optimizer = AdamWeightDecay(learning_rate=1e-3, weight_decay_rate=0.01)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-base')\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-base\", return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFYujptXc1Lq"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "\n",
        "#We don't need to separate the dataset in training and test manually\n",
        "# prepare_tf_dataset split automatically\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTPxnu6mc1Rt"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "history = model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVzPdhTNilXw"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61N4LwhJiyyc"
      },
      "source": [
        "**PREPROCESS REVIEWS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqBc0Am5itMl"
      },
      "outputs": [],
      "source": [
        "cleaned_reviews = read_list_from_file(filename_review_list)\n",
        "print(cleaned_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h-zbhz0illj"
      },
      "outputs": [],
      "source": [
        "len(cleaned_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuItYCu5jAan"
      },
      "outputs": [],
      "source": [
        "reviews_list_preprocessed = read_list_from_file(filename_review_list)\n",
        "batches = get_batches(reviews_list_preprocessed, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRw4g5S7jAd0"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  # print(decoded_output)\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62tXhwCKjFrY"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEvlJNIIjF_-"
      },
      "outputs": [],
      "source": [
        "batches[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uwEJEWvRZnZ"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uZ5rZg1RZ_o"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbvdcFy2RnPX"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycv9R7YbRyPT"
      },
      "source": [
        "**T5 base finetuning large**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn6UyWXzVokr"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGIynzu8Voyg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_nWunS8R6qW"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(filename_dataset, on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyA6BvGeSCfp"
      },
      "outputs": [],
      "source": [
        "amazon_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cru5k5XgSCiM"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "history = model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRGmzZH6SCll"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpxuw1ErUVn7"
      },
      "source": [
        "**test training 4000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWGC9dqwWkEv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_dataset_from_product_id(df, product_id, num_samples, test_size, random_state):\n",
        "    product_reviews = df[df['product_id'] == product_id]\n",
        "    if len(product_reviews) == 0:\n",
        "        raise ValueError(\"No reviews found for product_id: {}\".format(product_id))\n",
        "\n",
        "    obj_len = len(product_reviews)\n",
        "    if obj_len == 0:\n",
        "        raise ValueError(\"No reviews found for product_id: {}\".format(product_id))\n",
        "\n",
        "    train_size = int(obj_len * (1 - test_size))\n",
        "    train_indices = random_state.choice(obj_len, size=train_size, replace=False)\n",
        "    test_indices = np.setdiff1d(np.arange(obj_len), train_indices)\n",
        "\n",
        "    train_set = product_reviews.iloc[train_indices]\n",
        "    test_set = product_reviews.iloc[test_indices]\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "# Example usage\n",
        "amazon_reviews = pd.read_csv(filename_dataset, on_bad_lines=\"skip\")\n",
        "random_state = np.random.default_rng(42)\n",
        "train_set, test_set = get_dataset_from_product_id(amazon_reviews, \"B000YDDF6O\", 4000, 0.2, random_state)\n",
        "print(train_set)\n",
        "print(test_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZXRYVM7TJb-"
      },
      "outputs": [],
      "source": [
        "cleaned_reviews = read_list_from_file(filename_review_list)\n",
        "print(cleaned_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4r7zlFyTJrU"
      },
      "outputs": [],
      "source": [
        "len(cleaned_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FhvnxeYTROt"
      },
      "outputs": [],
      "source": [
        "reviews_list_preprocessed = read_list_from_file(filename_review_list)\n",
        "batches = get_batches(reviews_list_preprocessed, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRa92LolTShZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyDTz0aITRRR"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  # print(decoded_output)\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5VzZvbZTRUp"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsoufL9cTZts"
      },
      "outputs": [],
      "source": [
        "batches[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P4cvcwkTate"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_nfzflSTjgr"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19f4VrDXTmO5"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2gRFvX8UG2f"
      },
      "source": [
        "**T5 Large finetuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNhw-9HjX8go"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw5pwT0HX8sB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUrUwdypX81Z"
      },
      "outputs": [],
      "source": [
        "#Filename of the review list's file\n",
        "filename_review_list = \"/content/review_list.txt\"\n",
        "\n",
        "#Filename of the dataset\n",
        "filename_dataset = \"/content/Reduced DataSet.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itbyFJn4Yc81"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(filename_dataset, on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpsigOZgYdK4"
      },
      "outputs": [],
      "source": [
        "amazon_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9I85mhGYdUd"
      },
      "outputs": [],
      "source": [
        "def get_dataset_from_product_id(df, product_id, number_of_samples, test_size, shuffle):\n",
        "  csv_r = df[df[\"product_id\"] == product_id]\n",
        "  csv_r = csv_r[[\"review_body\", \"review_headline\"]]\n",
        "  csv_r = csv_r[csv_r[\"review_headline\"] != \"Five Stars\"]\n",
        "\n",
        "  #TODO\n",
        "  # review_body and review_headline must be preprocessed\n",
        "\n",
        "  dataset = Dataset.from_pandas(df = csv_r.sample(number_of_samples))\n",
        "  dataset = dataset.train_test_split(test_size = test_size)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PosPcH4SYdXs"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # inputs = [prefix + doc for doc in examples[\"review_body\"]]\n",
        "    inputs = [doc for doc in examples[\"review_body\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"review_headline\"], max_length=256, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMsVDPfQYt1u"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  i = np.argmin(val_loss)\n",
        "  x_min = epochs[i]\n",
        "  y_min = val_loss[i]\n",
        "  plt.plot(x_min, y_min,'g',marker='o', label=\"Minimum validation loss\")\n",
        "\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv6UTIy2Yt4X"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "  #creo un array di testi concatenati, ognuno dei quali ha massimo 1024 token\n",
        "  #lo stesso procedimento verrà fatto con i riassunti generati!\n",
        "  # si parte da checkpoint in poi, fino a finire il ciclo.\n",
        "\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FpGLZQ5Y3DH"
      },
      "source": [
        "T5 LARGE Training 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jyd2AJoYt7x"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset_from_product_id(amazon_reviews, \"B000YDDF6O\", 1000, 0.2, True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNOQ4hIWZPq5"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('t5-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBIiZ3zPZSx4"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "print(tokenized_dataset)\n",
        "#Maybe an error!! We've review_body, review headline, input_ids, ... but we need just input ids and labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd6RlhP2ZS0w"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "optimizer = AdamWeightDecay(learning_rate=1e-3, weight_decay_rate=0.01)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-large')\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-large\", return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ywUZ4EZS3T"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "\n",
        "#We don't need to separate the dataset in training and test manually\n",
        "# prepare_tf_dataset split automatically\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=1,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=True,\n",
        "    batch_size=1,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeUf5O8VZS5h"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "history = model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oji-vAQuZS9I"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVyrbcFRDcUF"
      },
      "source": [
        "**Final Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhXizaPLDPP5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import math\n",
        "import string\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWhHicoMDPgC"
      },
      "outputs": [],
      "source": [
        "#Load the dataset as csv, skipping the bad lines\n",
        "csv = pd.read_csv(\"/content/Reduced DataSet.csv\", on_bad_lines=\"skip\")\n",
        "\n",
        "#Drop the null values\n",
        "csv.dropna(inplace=True)\n",
        "\n",
        "#Check the number of the reviews for each product in the reduced dataset\n",
        "csv_grouped = csv.groupby(by=\"product_id\").count().sort_values(by=\"review_body\")\n",
        "csv_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYZZv8AWDPuH"
      },
      "outputs": [],
      "source": [
        "csv.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06bGa_fAEHcf"
      },
      "source": [
        "**DATASET CREATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO64GcG6EL3q"
      },
      "outputs": [],
      "source": [
        "csv_r = csv[csv[\"product_id\"] == \"B000YDDF6O\"]\n",
        "csv_r = csv_r[[\"review_body\", \"review_headline\", \"star_rating\",\"helpful_votes\",\"review_date\"]]\n",
        "csv_r.reset_index(inplace=True, drop=True)\n",
        "csv_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM9iDBvAEXoY"
      },
      "outputs": [],
      "source": [
        "# csv_r[\"review_weight\"] = math.log(csv_r[\"helpful_votes\"]+1)*csv_r[\"star_rating\"]\n",
        "\n",
        "csv_r[\"review_weight\"] = csv_r.apply(lambda x: math.log(x[\"helpful_votes\"]+1)*x[\"star_rating\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ke_V3ebEZY5"
      },
      "outputs": [],
      "source": [
        "csv_r[csv_r[\"review_weight\"] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r_f9xpEEZcP"
      },
      "outputs": [],
      "source": [
        "sum(csv_r[\"star_rating\"])/csv_r['star_rating'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe98Mzv4Eg3h"
      },
      "outputs": [],
      "source": [
        "\n",
        "#weighted average based on helpful votes\n",
        "sum(csv_r[csv_r[\"review_weight\"] != 0][\"review_weight\"])/sum(csv_r[csv_r[\"review_weight\"] != 0].apply(lambda x: math.log(x['helpful_votes']+1), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SMZKHrfEg62"
      },
      "outputs": [],
      "source": [
        "sum(csv_r[csv_r[\"review_weight\"] != 0][\"review_weight\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hocNMijCEqJM"
      },
      "outputs": [],
      "source": [
        "sum(csv_r[csv_r[\"review_weight\"] != 0].apply(lambda x: math.log(x['helpful_votes']+1), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LijdWDcxEqMm"
      },
      "outputs": [],
      "source": [
        "\n",
        "#weighted average based on helpful votes\n",
        "sum(csv_r['star_rating']*(csv_r['helpful_votes']+0.1))/sum(csv_r['helpful_votes']+0.1) #Adding significance to the ones with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQldRi52Evdb"
      },
      "outputs": [],
      "source": [
        "\n",
        "#weighted average based on helpful votes\n",
        "sum(csv_r['star_rating']*(csv_r['helpful_votes']))/sum(csv_r['helpful_votes']) #Without significance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRSSXp2EvvT"
      },
      "outputs": [],
      "source": [
        "\n",
        "#weighted average based on helpful votes logarithmic scale considering only those with weight != 0\n",
        "sum(csv_r[csv_r[\"review_weight\"] != 0][\"review_weight\"])/sum(csv_r[csv_r[\"review_weight\"] != 0].apply(lambda x: math.log(x['helpful_votes']+1), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_ABQsPAE1fQ"
      },
      "outputs": [],
      "source": [
        "csv_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TInOS697E8DB"
      },
      "outputs": [],
      "source": [
        "#Save the sample into a CSV file\n",
        "csv_r.to_csv('/content/sample_weighted.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROWCZy5gGGEt"
      },
      "source": [
        "BART FineTuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qS83GmGC2y"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HTewKTTGDIu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrElVPmaGDQC"
      },
      "outputs": [],
      "source": [
        "#Function to read a list from a file\n",
        "def read_list_from_file(filename):\n",
        "  result = []\n",
        "  # opening the file in read mode\n",
        "  my_file = open(filename, \"r\")\n",
        "\n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "\n",
        "  # replacing end splitting the text\n",
        "  # when newline ('\\n') is seen.\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "  return data_into_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c4EmJeHGDSa"
      },
      "outputs": [],
      "source": [
        "#Filename of the review list's file\n",
        "filename_review_list = \"/content/review_list.txt\"\n",
        "\n",
        "#Filename of the dataset\n",
        "filename_dataset = \"/content/Reduced DataSet.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pSixPfCGDVp"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(\"/content/Reduced DataSet.csv\", on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiDCXIP9GVrL"
      },
      "outputs": [],
      "source": [
        "amazon_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPYBEpATGVuv"
      },
      "outputs": [],
      "source": [
        "def get_dataset_from_product_id(df, product_id, number_of_samples, test_size, shuffle):\n",
        "  csv_r = df[df[\"product_id\"] == product_id]\n",
        "  csv_r = csv_r[[\"review_body\", \"review_headline\"]]\n",
        "\n",
        "  #TODO\n",
        "  # review_body and review_headline must be preprocessed\n",
        "\n",
        "  dataset = Dataset.from_pandas(df = csv_r[:number_of_samples])\n",
        "  dataset = dataset.train_test_split(test_size = test_size)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWOUEK7yGWJd"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset_from_product_id(amazon_reviews, \"B000YDDF6O\", 1000, 0.2, True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDWpP_7eOtXq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBjJpR_CGWM8"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftJMyvnNGgEA"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # inputs = [prefix + doc for doc in examples[\"review_body\"]]\n",
        "    inputs = [doc for doc in examples[\"review_body\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"review_headline\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY60bVO9GgQA"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "print(tokenized_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvNbZysgGgZY"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "optimizer = AdamWeightDecay(learning_rate=1e-3, weight_decay_rate=0.01)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"facebook/bart-base\", return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2NnOns3GmTC"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "\n",
        "#We don't need to separate the dataset in training and test manually\n",
        "# prepare_tf_dataset split automatically\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=True,\n",
        "    batch_size=4,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubyGsVUGGmgq"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "history = model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjBX1wEbGrQa"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  i = np.argmin(val_loss)\n",
        "  x_min = epochs[i]\n",
        "  y_min = val_loss[i]\n",
        "  plt.plot(x_min, y_min,'g',marker='o', label=\"Minimum validation loss\")\n",
        "\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A646OGGiGrer"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5o5B8vuGriO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "cleaned_reviews = read_list_from_file(filename_review_list)\n",
        "print(cleaned_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yUmStVMG0fD"
      },
      "outputs": [],
      "source": [
        "input = \"\"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "\n",
        "\n",
        "#batch creation\n",
        "\n",
        "batches = []\n",
        "i = 0\n",
        "for i in range(0,43):\n",
        "  if count + len(cleaned_reviews[i].split(\" \")) < 800:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + cleaned_reviews[i] + \" \"\n",
        "    #print(input)\n",
        "    count = count + len(cleaned_reviews[i].split(\" \"))\n",
        "  else:\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"\" + cleaned_reviews[i] + \" . \"\n",
        "    count = len(cleaned_reviews[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMVhsGnPG0s6"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  print(decoded_output)\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOAJXKNVG05W"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kyhLMtiG6oE"
      },
      "outputs": [],
      "source": [
        "batches[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZIdnZe4G62N"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "011BbLPIG7GD"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNDKANtnHEA5"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmb2sgP5HELU"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-xsum\", tokenizer=\"facebook/bart-large-xsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTcO3nwqHEWM"
      },
      "outputs": [],
      "source": [
        "text = batches[0]\n",
        "summary = summarizer( text, max_length=1024, min_length=10, do_sample=False)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ7QgFncHEZe"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqKvklzQKjsN"
      },
      "source": [
        "**BART LARGE XSUM**\n",
        "\n",
        "Second test: Introduction\n",
        "In the second test with BART we'll use a more complex model trained bart-large-xsum specialized for tasks of summarization.\n",
        "\n",
        "As before we use the following alternative inputs:\n",
        "\n",
        "Preprocessed reviews\n",
        "Full text\n",
        "We'll leverage on the huggingface's transformers library also in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqt5QJJaKm9t"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "getBLeEyK0wW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNHglwraK082"
      },
      "outputs": [],
      "source": [
        "#Function to read a list from a file\n",
        "def read_list_from_file(filename):\n",
        "  result = []\n",
        "  # opening the file in read mode\n",
        "\n",
        "  my_file = open(filename, \"r\")\n",
        "\n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "\n",
        "  # replacing end splitting the text\n",
        "  # when newline ('\\n') is seen.\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "  return data_into_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYwsk5AYK1BH"
      },
      "outputs": [],
      "source": [
        "#Filename of the review list's file\n",
        "filename_review_list = \"/content/review_list.txt\"\n",
        "\n",
        "#Filename of the dataset\n",
        "filename_dataset = \"/content/Reduced DataSet.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bGnYjmNK1Ee"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-xsum\", tokenizer=\"facebook/bart-large-xsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNNvnADYK1HS"
      },
      "outputs": [],
      "source": [
        "#These function receive as input a batch of reviews and for each one it outputs\n",
        "# a number of summaries equal to the number of batches\n",
        "def get_summaries(batches):\n",
        "  summaries = []\n",
        "  for input in batches:\n",
        "    summary = summarizer( input, max_length=1024, min_length=10, do_sample=False)\n",
        "    summaries.append(summary)\n",
        "  return summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQUcI3A3K_OQ"
      },
      "source": [
        "Creating the inputs\n",
        "\n",
        "In the following cell we can see how the inputs are created; in particular we build batches of reviews, each one formed by the concatenation of different reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBL_P_mJK1KM"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \" \"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "\n",
        "\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < (512):\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = reviews[i] + \" \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluB8SK5LHi0"
      },
      "source": [
        "Now we create the batches for the summaries\n",
        "\n",
        "The previous batches of reviews form the input to the first stage of summarization: the results will be composed by N summaries, where N is the number of inputs to the model (the size of batches)\n",
        "\n",
        "Loading the preprocessed reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwcoAyP5LFXY"
      },
      "outputs": [],
      "source": [
        "\n",
        "reviews_list_preprocessed = read_list_from_file(filename_review_list)\n",
        "print(reviews_list_preprocessed[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRZy7F1LLFbC"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(reviews_list_preprocessed, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTy2Ru-vLFeH"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(\"/content/amazon_reviews_reduced_preprocessed.csv\", on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuRSJ_VoLFhs"
      },
      "outputs": [],
      "source": [
        "review_list = amazon_reviews[\"review_body\"].tail(1000).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bd9htrRLFk-"
      },
      "outputs": [],
      "source": [
        "batches_full_text = get_batches(review_list, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRlx4-KqLbhI"
      },
      "source": [
        "Using the preprocessed reviews\n",
        "\n",
        "The previous batches of reviews form the input to the first stage of summarization: the results will be composed by N summaries, where N is the number of inputs to the model (the size of batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-BHI1G9LFo4"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = get_summaries(batches=batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD6sqq85LhG3"
      },
      "outputs": [],
      "source": [
        "len(batches[0].split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j34TVDQqLhKf"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = [x[0][\"summary_text\"] for x in summaries_first_stage]\n",
        "summaries_first_stage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuVbuzWKLhOY"
      },
      "outputs": [],
      "source": [
        "print(summaries_first_stage)\n",
        "print(batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1BxXNTELhRg"
      },
      "outputs": [],
      "source": [
        "batches_second_stage = get_batches(summaries_first_stage, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuDEkKjzLhVs"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = get_summaries(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHqPZceGLu-X"
      },
      "outputs": [],
      "source": [
        "print(summaries_second_stage[0])\n",
        "print(summaries_first_stage[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L579s-N8LvB2"
      },
      "outputs": [],
      "source": [
        "#Sentiment analysis\n",
        "#Bert-base-multilingual-uncased-sentiment is a model fine-tuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian.\n",
        "pipe = pipeline(model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aRRZRFwLvFr"
      },
      "outputs": [],
      "source": [
        "#replace the specific product\n",
        "\n",
        "review_stars = pd.read_csv(\"/content/Reduced DataSet.csv\", on_bad_lines=\"skip\")\n",
        "mask = review_stars[review_stars[\"product_id\"] == \"B000YDDF6O\"]\n",
        "mask = mask[\"star_rating\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "799SRWACL1ra"
      },
      "outputs": [],
      "source": [
        "val = pipe(summaries_first_stage[0])\n",
        "print(\"First Stage: \" + str(val[0].get(\"score\")))\n",
        "\n",
        "val = pipe(summaries_second_stage[0][0].get(\"summary_text\"))\n",
        "print(\"Second Stage: \" + str(val[0].get(\"score\")))\n",
        "\n",
        "print(\"Expected Rating: \" + str((mask.sum()/len(mask))/5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRtxnYAmL6OD"
      },
      "source": [
        "Using the full text without preprocessing\n",
        "\n",
        "\n",
        "The second part of the test on BART-base consists in the usage of the full text (without preprocessing) as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCaW9tFUL16Q"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage_full_text = get_summaries(batches=batches_full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AumwoLNSL1_E"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage_full_text = [x[0][\"summary_text\"] for x in summaries_first_stage_full_text]\n",
        "summaries_first_stage_full_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MazO6D3sMFiR"
      },
      "outputs": [],
      "source": [
        "batches_second_stage_full_text = get_batches(summaries_first_stage_full_text, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE9_ZrxWMFmp"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage_full_text = get_summaries(batches_second_stage_full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxMAT3hAMJEc"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage_full_text = [x[0][\"summary_text\"] for x in summaries_second_stage_full_text]\n",
        "summaries_second_stage_full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUzmOnGrMJIQ"
      },
      "outputs": [],
      "source": [
        "print(summaries_second_stage_full_text[0])\n",
        "print(summaries_first_stage_full_text[0])\n",
        "print(batches_full_text[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY-xVy8pMOi4"
      },
      "outputs": [],
      "source": [
        "#Sentiment analysis\n",
        "#Bert-base-multilingual-uncased-sentiment is a model fine-tuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian.\n",
        "pipe = pipeline(model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVdLZf2TMRfd"
      },
      "outputs": [],
      "source": [
        "review_stars = pd.read_csv(\"/content/Reduced DataSet.csv\", on_bad_lines=\"skip\")\n",
        "mask = review_stars[review_stars[\"product_id\"] == \"B000YDDF6O\"]\n",
        "mask = mask[\"star_rating\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA4qIGzOMRjS"
      },
      "outputs": [],
      "source": [
        "val = pipe(summaries_first_stage_full_text[0])\n",
        "print(\"First Stage: \" + str(val[0].get(\"score\")))\n",
        "\n",
        "val = pipe(summaries_second_stage_full_text)\n",
        "print(\"Second Stage: \" + str(val[0].get(\"score\")))\n",
        "\n",
        "print(\"Expected Rating: \" + str((mask.sum()/len(mask))/5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGEzkGOyRSwr"
      },
      "source": [
        "Third test: Introduction\n",
        "\n",
        "In the second test with BART we'll use a more complex model trained bart-large-cnn specialized for tasks of summarization.\n",
        "\n",
        "As before we use the following alternative inputs:\n",
        "\n",
        "Preprocessed reviews\n",
        "Full text\n",
        "We'll leverage on the huggingface's transformers library also in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Igj5R5kIMRnS"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYIsgWOkRcU_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL_OARbHRcX3"
      },
      "outputs": [],
      "source": [
        "#Function to read a list from a file\n",
        "def read_list_from_file(filename):\n",
        "  result = []\n",
        "  # opening the file in read mode\n",
        "  my_file = open(filename, \"r\")\n",
        "\n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "\n",
        "  # replacing end splitting the text\n",
        "  # when newline ('\\n') is seen.\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "  return data_into_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSlKMWUvRcay"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij9JntOZRn6Y"
      },
      "source": [
        "Defining the model\n",
        "\n",
        "For this test we'll use bart-large-cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BCjFONHRcds"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G2HP5QqRcgg"
      },
      "outputs": [],
      "source": [
        "#These function receive as input a batch of reviews and for each one it outputs\n",
        "# a number of summaries equal to the number of batches\n",
        "def get_summaries(batches):\n",
        "  summaries = []\n",
        "  for input in batches:\n",
        "    summary = summarizer( input, max_length=1024, min_length=10, do_sample=False)\n",
        "    summaries.append(summary)\n",
        "  return summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SttGlzRIRwQx"
      },
      "source": [
        "Creating the inputs\n",
        "\n",
        "In the following cell we can see how the inputs are created; in particular we build batches of reviews, each one formed by the concatenation of different reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s32K4A0bRcjP"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \" \"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < (512):\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = reviews[i] + \" \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mUq_R4TR7vt"
      },
      "source": [
        "Now we create the batches for the summaries\n",
        "\n",
        "\n",
        "The previous batches of reviews form the input to the first stage of summarization: the results will be composed by N summaries, where N is the number of inputs to the model (the size of batches)\n",
        "\n",
        "Loading the preprocessed reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWuWd2vyRcmB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "reviews_list_preprocessed = read_list_from_file(filename_review_list)\n",
        "print(reviews_list_preprocessed[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTh32jEIRcox"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(reviews_list_preprocessed, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlqXp6O3Rcrw"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(\"/content/amazon_reviews_reduced_preprocessed.csv\", on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpYFVjCDRcur"
      },
      "outputs": [],
      "source": [
        "review_list = amazon_reviews[\"review_body\"].tail(1000).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSVtfmPMRcxv"
      },
      "outputs": [],
      "source": [
        "batches_full_text = get_batches(review_list, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCPZENrUSI63"
      },
      "source": [
        "Using the preprocessed reviews\n",
        "\n",
        "The previous batches of reviews form the input to the first stage of summarization: the results will be composed by N summaries, where N is the number of inputs to the model (the size of batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsJtjm8LRc0n"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = get_summaries(batches=batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu2tJEMpRc3n"
      },
      "outputs": [],
      "source": [
        "len(batches[0].split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVuagMDVRc7F"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = [x[0][\"summary_text\"] for x in summaries_first_stage]\n",
        "summaries_first_stage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtXSfZleRc9s"
      },
      "outputs": [],
      "source": [
        "print(summaries_first_stage)\n",
        "print(batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6r3XrF4RdAm"
      },
      "outputs": [],
      "source": [
        "batches_second_stage = get_batches(summaries_first_stage, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgO6c-2LRdDY"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = get_summaries(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9xjiD0HRdHa"
      },
      "outputs": [],
      "source": [
        "print(summaries_second_stage[0])\n",
        "print(summaries_first_stage[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StIjLWJ1Savb"
      },
      "source": [
        "Using the full text without preprocessing\n",
        "\n",
        "The second part of the test on BART-base consists in the usage of the full text (without preprocessing) as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnhqBi6ORdJn"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage_full_text = get_summaries(batches=batches_full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P_uPwQxRdMs"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage_full_text = [x[0][\"summary_text\"] for x in summaries_first_stage_full_text]\n",
        "summaries_first_stage_full_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDriTAtxRdPk"
      },
      "outputs": [],
      "source": [
        "batches_second_stage_full_text = get_batches(summaries_first_stage_full_text, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sekr52x-RdSs"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage_full_text = get_summaries(batches_second_stage_full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmR7KO3DRdVm"
      },
      "outputs": [],
      "source": [
        "print(summaries_first_stage_full_text[0])\n",
        "print(batches_full_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNqhtAIzXLzU"
      },
      "source": [
        "**PEGASUS BASE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfhBoWf-RdYb"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ6cuMBQRdbl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQec4CNmRdfP"
      },
      "outputs": [],
      "source": [
        "#Function to read a list from a file\n",
        "def read_list_from_file(filename):\n",
        "  result = []\n",
        "  # opening the file in read mode\n",
        "  my_file = open(filename, \"r\")\n",
        "\n",
        "  # reading the file\n",
        "  data = my_file.read()\n",
        "\n",
        "  # replacing end splitting the text\n",
        "  # when newline ('\\n') is seen.\n",
        "  data_into_list = data.split(\"\\n\")\n",
        "  my_file.close()\n",
        "\n",
        "  return data_into_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51ocxKlIXWww"
      },
      "outputs": [],
      "source": [
        "#Filename of the review list's file\n",
        "filename_review_list = \"/content/review_list.txt\"\n",
        "\n",
        "#Filename of the dataset\n",
        "filename_dataset = \"/content/Reduced DataSet.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ek3OmvMXWzs"
      },
      "outputs": [],
      "source": [
        "amazon_reviews = pd.read_csv(filename_dataset, on_bad_lines=\"skip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzpjeUt8XW2m"
      },
      "outputs": [],
      "source": [
        "amazon_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mWaVdYrXW5P"
      },
      "outputs": [],
      "source": [
        "def get_dataset_from_product_id(df, product_id, number_of_samples, test_size, shuffle):\n",
        "  csv_r = df[df[\"product_id\"] == product_id]\n",
        "  csv_r = csv_r[[\"review_body\", \"review_headline\"]]\n",
        "  csv_r = csv_r[~csv_r[\"review_headline\"].isin([\"Five Stars\", \"Four Stars\", \"Three Stars\", \"Two Stars\", \"One Star\"])]\n",
        "\n",
        "\n",
        "  dataset = Dataset.from_pandas(df = csv_r.sample(number_of_samples))\n",
        "  dataset = dataset.train_test_split(test_size = test_size)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVlQE-5SXW76"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # inputs = [prefix + doc for doc in examples[\"review_body\"]]\n",
        "    inputs = [doc for doc in examples[\"review_body\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"review_headline\"], max_length=256, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyIGCUtOXW-h"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "\n",
        "  i = np.argmin(val_loss)\n",
        "  x_min = epochs[i]\n",
        "  y_min = val_loss[i]\n",
        "  plt.plot(x_min, y_min,'g',marker='o', label=\"Minimum validation loss\")\n",
        "\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVrsph5mXXBP"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzohj8gfXo09"
      },
      "source": [
        "**TEST  Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5TelxvPXXD8"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset_from_product_id(amazon_reviews, \"B000YDDF6O\", 4000, 0.2, True)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvrZr2ycXXGj"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tANy7sSYXXJK"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "print(tokenized_dataset)\n",
        "#Maybe an error!! We've review_body, review headline, input_ids, ... but we need just input ids and labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvIp-RqHX1je"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "optimizer = AdamWeightDecay(learning_rate=1e-3, weight_decay_rate=0.01)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google/pegasus-xsum')\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"google/pegasus-xsum\", return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv4yo8ENX1wU"
      },
      "outputs": [],
      "source": [
        "# SPLIT DATASET\n",
        "\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=2,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    shuffle=True,\n",
        "    batch_size=2,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciHw_vOfX1zU"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer)\n",
        "history = model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enfkFn2WX111"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEZzjkr1X14y"
      },
      "outputs": [],
      "source": [
        "#load the preprocessed reviews BUT!! We've pretrained the network on the same ones, it's an error\n",
        "# maybe we don't need the preprocessing or we can skip the first 1000 reviews\n",
        "\n",
        "cleaned_reviews = read_list_from_file(filename_review_list)\n",
        "print(cleaned_reviews[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqL9Lpt-X17M"
      },
      "outputs": [],
      "source": [
        "len(cleaned_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyITi1sWX1-H"
      },
      "outputs": [],
      "source": [
        "reviews_list_preprocessed = read_list_from_file(filename_review_list)\n",
        "batches = get_batches(reviews_list_preprocessed, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scBbMqZVYD7l"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  # print(decoded_output)\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irp0MzadYD-a"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0HkRflvYEBR"
      },
      "outputs": [],
      "source": [
        "batches[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_ylkwEiYED8"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx0SDPlqYEHS"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FODQXRgeYNj_"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Ns0vSrYYOi"
      },
      "source": [
        "DATASET SPLIT BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHiKnGuKYN3R"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ecf7AgYiC8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMP9JxnzYiGD"
      },
      "outputs": [],
      "source": [
        "filename_dataset = \"/content/sample_weighted.csv\"\n",
        "df = pd.read_csv(filename_dataset)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txNSgvMtYiJI"
      },
      "outputs": [],
      "source": [
        "date = df['review_date']\n",
        "date_list = []\n",
        "for d in date:\n",
        "  elem = d.split('-')\n",
        "  date_list.append(elem[0])\n",
        "date_list = [eval(i) for i in date_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYw85F6uYiLf"
      },
      "outputs": [],
      "source": [
        "plt.hist(date_list, bins=12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vi8CxNKYiOa"
      },
      "outputs": [],
      "source": [
        "df['review_date'] = date_list\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kSjwpuGYiRQ"
      },
      "outputs": [],
      "source": [
        "df_2010 = df[df['review_date'] == 2010]\n",
        "df_2011 = df[df['review_date'] == 2011]\n",
        "df_2012 = df[df['review_date'] == 2012]\n",
        "df_2013 = df[df['review_date'] == 2013]\n",
        "df_2014 = df[df['review_date'] == 2014]\n",
        "df_2015 = df[df['review_date'] == 2015]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-9dSq18YiTn"
      },
      "outputs": [],
      "source": [
        "df_2010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcqsOhGeYiWl"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W5Kd0gMYiaD"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zauVzLNRY26t"
      },
      "outputs": [],
      "source": [
        "reviews = df_2010['review_body'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVh2G6avY3Go"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(reviews, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CsECCyDY3Jz"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  # print(decoded_output)\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssWM5XhvY3Na"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3UTgOSTY3d3"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdeYTpYeY3gd"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNWAj1NFY3ju"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqpVAYtAZJ0I"
      },
      "source": [
        "DATASET SPLIT PEGASUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzN2VuziZIT3"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QloS1OyXZIjt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBXTAnr_ZInA"
      },
      "outputs": [],
      "source": [
        "filename_dataset = \"/content/sample_weighted.csv\"\n",
        "df = pd.read_csv(filename_dataset)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPMiyhn0ZI1o"
      },
      "outputs": [],
      "source": [
        "date = df['review_date']\n",
        "date_list = []\n",
        "for d in date:\n",
        "  elem = d.split('-')\n",
        "  date_list.append(elem[0])\n",
        "date_list = [eval(i) for i in date_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcfUVhsqZI4s"
      },
      "outputs": [],
      "source": [
        "plt.hist(date_list, bins=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-iRX91nZI8G"
      },
      "outputs": [],
      "source": [
        "df['review_date'] = date_list\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3AcwUiVZcRC"
      },
      "outputs": [],
      "source": [
        "df_2010 = df[df['review_date'] == 2010]\n",
        "df_2011 = df[df['review_date'] == 2011]\n",
        "df_2012 = df[df['review_date'] == 2012]\n",
        "df_2013 = df[df['review_date'] == 2013]\n",
        "df_2014 = df[df['review_date'] == 2014]\n",
        "df_2015 = df[df['review_date'] == 2015]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwt6kQMnZcUg"
      },
      "outputs": [],
      "source": [
        "df_2010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz96H9ReZcXM"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdpPvTXSZcac"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH5ELyPDZcdK"
      },
      "outputs": [],
      "source": [
        "reviews = df_2014['review_body'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTZkEvv3Zcgi"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(reviews, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfPT1rCzZ5U9"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage = []\n",
        "for input in batches:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  # print(decoded_output)\n",
        "  summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7na9VqnqL34"
      },
      "outputs": [],
      "source": [
        "max_input_length = 512\n",
        "\n",
        "summaries_first_stage = []\n",
        "for input_text in batches:\n",
        "    # Split the input into segments of max_input_length\n",
        "    segments = [input_text[i:i + max_input_length] for i in range(0, len(input_text), max_input_length)]\n",
        "\n",
        "    for segment in segments:\n",
        "        inputs = tokenizer.encode(segment, return_tensors='tf', truncation=True, max_length=max_input_length)\n",
        "        output = model.generate(inputs, max_length=100)\n",
        "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "        summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7QAS8iVZ8o7"
      },
      "outputs": [],
      "source": [
        "summaries_first_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVCwkfgTZ85_"
      },
      "outputs": [],
      "source": [
        "input = \"summarize: \"\n",
        "count = 0\n",
        "checkpoint = 0\n",
        "batches_second_stage = []\n",
        "i = 0\n",
        "for i in range(0,len(summaries_first_stage)):\n",
        "  if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "    #print(cleaned_reviews[i])\n",
        "    input = input + summaries_first_stage[i] + \" . \"\n",
        "    #print(input)\n",
        "    count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "  else:\n",
        "    batches_second_stage.append(input)\n",
        "    print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "    print(\"Number of reviews processed: {}\".format(i))\n",
        "    input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "    count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "if(count != 0):\n",
        "  batches_second_stage.append(input)\n",
        "  print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "  print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "print(batches_second_stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3kyy-yQZ89U"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage = []\n",
        "for input in batches_second_stage:\n",
        "  inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "  output = model.generate(inputs, max_length=100)\n",
        "  decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "  summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftuOheTkZ9At"
      },
      "outputs": [],
      "source": [
        "summaries_second_stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7tsmQWkdRNz"
      },
      "source": [
        "**FINAL TEST (0)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqUH5XPAcFxW"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5_ldqTeQNZT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/Reduced DataSet.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-LY9GzhcF-R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "from transformers import pipeline\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uli6ltIPcGBP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "from transformers import pipeline\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIM7YgMzcGEG"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "\n",
        "\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      #print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      #print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    #print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    #print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZOplh6gcGHA"
      },
      "outputs": [],
      "source": [
        "def summarize_final(df):\n",
        "  final_sum = []\n",
        "  ids = df['product_id'].unique()\n",
        "  years = df['review_date'].unique()\n",
        "  for year in years:\n",
        "      part = df[(df[\"product_id\"] == \"B000YDDF6O\") & (df[\"review_year\"] == year)]\n",
        "      reviews = part[\"review_body\"].tolist()\n",
        "      batches = get_batches(reviews, False)\n",
        "\n",
        "      summaries_first_stage = []\n",
        "      for input in batches:\n",
        "        inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "        output = model.generate(inputs, max_length=100)\n",
        "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "        # print(decoded_output)\n",
        "        summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])\n",
        "\n",
        "      input = \"summarize: \"\n",
        "      count = 0\n",
        "      checkpoint = 0\n",
        "      batches_second_stage = []\n",
        "      i = 0\n",
        "      for i in range(0,len(summaries_first_stage)):\n",
        "        if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "          #print(cleaned_reviews[i])\n",
        "          input = input + summaries_first_stage[i] + \" . \"\n",
        "          #print(input)\n",
        "          count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "        else:\n",
        "          batches_second_stage.append(input)\n",
        "          #print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "          #print(\"Number of reviews processed: {}\".format(i))\n",
        "          input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "          count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "      if(count != 0):\n",
        "        batches_second_stage.append(input)\n",
        "        #print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "        #print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "      summaries_second_stage = []\n",
        "      for input in batches_second_stage:\n",
        "        inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "        output = model.generate(inputs, max_length=100)\n",
        "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "        summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])\n",
        "\n",
        "      print(\"Product \" + str(id) + \", year \" + str(year) + \": \" + str(summaries_second_stage))\n",
        "      if(len(summaries_second_stage) != 0):\n",
        "        final_sum.append(summaries_second_stage)\n",
        "  return final_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1qoI8-0cGKP"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEAAAdDucGMx"
      },
      "outputs": [],
      "source": [
        "summaries = summarize_final(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NOSYfihcGPv"
      },
      "outputs": [],
      "source": [
        "ds = df.groupby(by=['product_id', 'review_year'], sort=True).mean()\n",
        "ds = ds.sort_values(by=['product_id', 'review_year'], ascending=[True, True])\n",
        "ds = ds['star_rating'].tolist()\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftj9ul-VcGS1"
      },
      "outputs": [],
      "source": [
        "#Sentiment analysis\n",
        "#Bert-base-multilingual-uncased-sentiment is a model fine-tuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian.\n",
        "pipe = pipeline(model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVQ1OZyjcGWU"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for sum in summaries:\n",
        "  val = pipe(str(sum))\n",
        "  print(\"Second Stage: \" + str(val[0].get(\"score\")))\n",
        "  print(\"Expected Rating: \" + str(ds[i]/5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQclqJZmd6-A"
      },
      "source": [
        "**FINAL TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA5fav8tcGsN"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXnmJ7-_cGvK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, TrainingArguments, Trainer, create_optimizer, AdamWeightDecay, DataCollatorForSeq2Seq\n",
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import os\n",
        "from numba import cuda\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPYdGFtzcGyA"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/Reduced DataSet.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjsN4TiDu2bB"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1uwEbKRcG0r"
      },
      "outputs": [],
      "source": [
        "def get_batches(reviews, sample):\n",
        "\n",
        "  input = \"summarize:\"\n",
        "  count = 0\n",
        "  checkpoint = 0\n",
        "\n",
        "\n",
        "  #batch creation\n",
        "\n",
        "\n",
        "  batches = []\n",
        "  i = 0\n",
        "\n",
        "  if sample:\n",
        "    num_samples = 100\n",
        "  else:\n",
        "    num_samples = len(reviews)\n",
        "\n",
        "  for i in range(0,num_samples):\n",
        "  # for i in range(0,43):\n",
        "    if count + len(reviews[i].split(\" \")) < 1024:\n",
        "      #print(cleaned_reviews[i])\n",
        "      input = input + reviews[i] + \" . \"\n",
        "      #print(input)\n",
        "      count = count + len(reviews[i].split(\" \"))\n",
        "    else:\n",
        "      batches.append(input)\n",
        "      #print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "      #print(\"Number of reviews processed: {}\".format(i))\n",
        "      input = \"summarize: \" + reviews[i] + \" . \"\n",
        "      count = len(reviews[i].split(\" \"))\n",
        "\n",
        "  if(count != 0):\n",
        "    batches.append(input)\n",
        "    #print('Input {} created with a length of {} tokens.'.format(len(batches), count))\n",
        "    #print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "  return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIwuyLhpcG3o"
      },
      "outputs": [],
      "source": [
        "def summarize_final(df):\n",
        "  ids = df['product_id'].unique()\n",
        "  years = df['review_year'].unique()\n",
        "  for id in ids:\n",
        "    for year in years:\n",
        "      part = df[(df[\"product_id\"] == id) & (df[\"review_year\"] == year)]\n",
        "      reviews = part[\"review_body\"].tolist()\n",
        "      batches = get_batches(reviews, False)\n",
        "\n",
        "      summaries_first_stage = []\n",
        "      for input in batches:\n",
        "        inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "        output = model.generate(inputs, max_length=100)\n",
        "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "        # print(decoded_output)\n",
        "        summaries_first_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])\n",
        "\n",
        "      input = \"summarize: \"\n",
        "      count = 0\n",
        "      checkpoint = 0\n",
        "      batches_second_stage = []\n",
        "      i = 0\n",
        "      for i in range(0,len(summaries_first_stage)):\n",
        "        if count + len(summaries_first_stage[i].split(\" \")) < 1024:\n",
        "          #print(cleaned_reviews[i])\n",
        "          input = input + summaries_first_stage[i] + \" . \"\n",
        "          #print(input)\n",
        "          count = count + len(summaries_first_stage[i].split(\" \"))\n",
        "        else:\n",
        "          batches_second_stage.append(input)\n",
        "          #print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "          #print(\"Number of reviews processed: {}\".format(i))\n",
        "          input = \"summarize: \" + summaries_first_stage[i] + \" . \"\n",
        "          count = len(summaries_first_stage[i].split(\" \"))\n",
        "\n",
        "      if(count != 0):\n",
        "        batches_second_stage.append(input)\n",
        "        #print('Input {} created with a length of {} tokens.'.format(len(batches_second_stage), count))\n",
        "        #print(\"Number of reviews processed: {}\".format(i))\n",
        "\n",
        "      summaries_second_stage = []\n",
        "      for input in batches_second_stage:\n",
        "        inputs  = tokenizer.encode(input, return_tensors='tf', truncation=True, max_length=1024)\n",
        "        output = model.generate(inputs, max_length=100)\n",
        "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "        summaries_second_stage.append(nltk.sent_tokenize(decoded_output.strip())[0])\n",
        "\n",
        "      print(\"Product \" + str(id) + \", year \" + str(year) + \": \" + str(summaries_second_stage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "010Xqh9jcG6S"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bYpn7NqcG9j"
      },
      "outputs": [],
      "source": [
        "summaries =summarize_final(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA5ZprWCur4A"
      },
      "outputs": [],
      "source": [
        "ds = df.groupby(by=['product_id', 'review_year'], sort=True).mean()\n",
        "ds = ds.sort_values(by=['product_id', 'review_year'], ascending=[True, True])\n",
        "ds = ds['star_rating'].tolist()\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSjf8DncusJc"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNLVpKxKusV-"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "for sum in summaries:\n",
        "  val = pipe(str(sum))\n",
        "  print(\"Second Stage: \" + str(val[0].get(\"score\")))\n",
        "  print(\"Expected Rating: \" + str(ds[i]/5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}